{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/heechul/EmbeddedML/blob/main/l5-vww/train-person.ipynb\">\n",
    "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRTa3Ee15WsJ"
   },
   "source": [
    "# Person-Detection using Transfer Learning\n",
    "In this assignment we will use transfer learning to detect if there is a human in a given image. We will use a pre-trained model (MobileNet-V2), which was trained on ImageNet dataset and use a [Kaggle human detection dataset](https://www.kaggle.com/datasets/constantinwerner/human-detection-dataset).\n",
    "\n",
    "**Acknowledgements**\n",
    "\n",
    "This colab is heavily borrowed from the [Google's Transfer learning](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb) Colab and [EdX TinyML course's](https://github.com/tinyMLx) Mask Detection Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!pip install -U tf_keras  &> 0 # Keras 2\n",
    "!pip install kagglehub\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "TqOt6Sv7AsMi"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "\n",
    "# Suppress only PNG warnings\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "RunningInCOLAB = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GoKGm1duzgk"
   },
   "source": [
    "## Build the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Qz0xYcXg4CX"
   },
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"constantinwerner/human-detection-dataset\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6B8zGs6JSAE4",
    "outputId": "16ce0736-02e2-4659-99e5-250210061001"
   },
   "outputs": [],
   "source": [
    "if RunningInCOLAB:\n",
    "    !mv {path} /content/data\n",
    "    !ls /content/data\n",
    "    path = \"/content/data\"\n",
    "else:\n",
    "    !mv {path} data\n",
    "    !ls data\n",
    "    path = \"./data\"    \n",
    "dataset_path = os.path.join(path, \"human detection dataset\")\n",
    "print (\"Path to dataset files:\", dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubMF9tn6jvba"
   },
   "outputs": [],
   "source": [
    "topdir = os.getcwd()\n",
    "if not os.path.exists('model'):\n",
    "    os.mkdir('model')\n",
    "if not os.path.exists('src'):\n",
    "    os.mkdir('src')\n",
    "print(topdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ro4oYaEmxe4r",
    "outputId": "aca753f8-e59d-46cc-e49d-60c0415a7874"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (96, 96)\n",
    "dataset = image_dataset_from_directory(dataset_path,\n",
    "                                        labels='inferred',\n",
    "                                        label_mode='binary',\n",
    "                                        shuffle=True,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        image_size=IMG_SIZE)\n",
    "\n",
    "dataset_size = len(list(dataset))  # Get the total number of elements in the dataset\n",
    "train_size = int(0.8 * dataset_size) # Calculate the size of the training set (80%)\n",
    "val_size = int(0.2 * dataset_size) # Calculate the size of the validation set (20%)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size).take(val_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yO1Q2JaW5sIy"
   },
   "source": [
    "Now that we have built the dataset lets view the first nine images and labels from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 829
    },
    "id": "K5BeQyKThC_Y",
    "outputId": "84968771-1207-4133-ce24-dc532fafd097"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "class_names = dataset.class_names\n",
    "print (class_names)\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in dataset.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    idx = labels[i].numpy().astype(\"uint8\")[0]\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[idx])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAywKtuVn8uK"
   },
   "source": [
    "### Rescale pixel values\n",
    "\n",
    "In a moment, you will download `tf.keras.applications.MobileNetV2` for use as your base model. This model expects pixel vaues in `[-1,1]`, but at this point, the pixel values in your images are in `[0-255]`. To rescale them, use the preprocessing method included with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "cO0HM9JAQUFq"
   },
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnr81qRMzcs5"
   },
   "source": [
    "Note: Alternatively, you could rescale pixel values from `[0,255]` to `[-1, 1]` using a [Rescaling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling) layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wz7qgImhTxw4"
   },
   "source": [
    "Note: If you go on to use other `tf.keras.applications`, be sure to check the API doc to determine if they expect pixels in `[-1,1]` or `[0,1]`, or use the included `preprocess_input` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptjsX-lTg7f3"
   },
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkH-kazQecHB"
   },
   "source": [
    "### Create the base model\n",
    "You will create the base model from the **MobileNet V2** model developed at Google. This is pre-trained on the ImageNet dataset, a large dataset consisting of 1.4M images and 1000 classes. ImageNet is a research training dataset with a wide variety of categories including objects such as `jackfruit` and `syringe`. This base of knowledge will help us classify if a person exists or not from our specific dataset.\n",
    "\n",
    "First, you need to pick which layer of MobileNet V2 you will leverage as the high level features you wish to re-use. Since we want to adapt the classifications coming out of the model to a new task, we want to leverage the features coming out of the *last* layer BEFORE the classification layers. In many image models this is the output of the final convolution BEFORE the flatten layer. You may see this layer referred to as the \"bottleneck layer\" in some texts. Since many machine learning models are defined as the inputs occuring at the bottom and the outputs occuring at the top we would like to ignore the top few classification layers. Fortuntately, there is a shortcut to doing this in TensorFlow, **```include_top=False```**. By passing in this parameter we instantiate a MobileNet V2 model pre-loaded with weights trained on ImageNet that doesn't include the classification layers at the top, which is ideal for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19IQ2gqneqmS",
    "outputId": "23365ded-5b34-4f0c-8424-32e97d6ac56f"
   },
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               alpha=0.35,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-2LJL0EEUcx",
    "outputId": "817fcbf5-31fd-47e6-8a22-cf2c82ec00ec"
   },
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(train_dataset))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnMLieHBCwil"
   },
   "source": [
    "### Freeze the convolutional layers\n",
    "\n",
    "It is important to freeze the convolutional layers before you compile and train the model with transfer learning. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. We want to perform this operation because we want to leverage the pre-trained values in the convolutional layers and only learn new classification layer values. We can do this by setting the entire model's `trainable` flag to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTCJH4bphOeo"
   },
   "outputs": [],
   "source": [
    "print (base_model.trainable)\n",
    "base_model.trainable = False\n",
    "print (base_model.trainable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KpbzSmPkDa-N",
    "outputId": "f7cf1d58-d4cf-4fe2-bddb-98899cfb317b"
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the base model architecture\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdMRM8YModbk"
   },
   "source": [
    "### Add a classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBc31c4tMOdH"
   },
   "source": [
    "To begin the process of generating classifications from the pretrained features, we use a `tf.keras.layers.GlobalAveragePooling2D` layer to convert the 3x3 spatial features into a single 1024-element feature vector per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLnpMF5KOALm",
    "outputId": "33ddb1fb-783a-4ab9-fc87-0e250d9ac8f5"
   },
   "outputs": [],
   "source": [
    "classification_input_layer = tf.keras.layers.Flatten()\n",
    "# classification_input_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "feature_batch_average = classification_input_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1p0OJBR6dOT"
   },
   "source": [
    "We then apply a `tf.keras.layers.Dense` layer to convert the feature vector into a single prediction per image. You don't need an activation function here because this prediction will be treated as a `logit`, or a raw prediction value.  Positive numbers predict class 1, negative numbers predict class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wv4afXKj6cVa",
    "outputId": "07c934ac-23f0-4901-e78f-e94f29de4157"
   },
   "outputs": [],
   "source": [
    "prediction_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXvz-ZkTa9b3"
   },
   "source": [
    "We can then build our final model by chaining together the data augmentation, rescaling, base_model and feature extractor layers using the [Keras Functional API](https://www.tensorflow.org/guide/keras/functional). Importantly we remind Tensorflow that we do not want to train the base_model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "DgzQX6Veb2WT"
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(96, 96, 3))\n",
    "x = preprocess_input(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = classification_input_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0ylJXE_kRLi"
   },
   "source": [
    "### Compile the model\n",
    "\n",
    "Compile the model before training it. Since there are two classes, use a binary cross-entropy loss with `from_logits=True` since the model provides a linear output. You can see below how we have included the Mobilenet V1 model after the input layer and before our classification layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpR8HdyMhukJ"
   },
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I8ARiyMFsgbH",
    "outputId": "5af4a6fc-35ae-4e80-f1a9-45e07c8552da"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxvgOYTDSWTx"
   },
   "source": [
    "## Your Turn: Train and Evaluate the model\n",
    "\n",
    "Now that we have our model we can train it! You will see that since we are leveraging all of the pre-trained features we can improve our model from a random initialization (accuracy of ~50%) to a model with over 95% accuracy quite quickly. **How many epochs do you think we'll need?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8906u9RQhR4Z"
   },
   "source": [
    "### Train the model\n",
    "First print the initial accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Om4O3EESkab1",
    "outputId": "9cf156d5-8b73-465b-d1d5-2eacdd29d702"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(val_dataset)\n",
    "print('Test accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQC4P6tOhbDm"
   },
   "source": [
    "**Now its your turn to pick the number of epochs of training! Remmeber we are aiming for at least 95% accuracy on the test set!**\n",
    "\n",
    "*Hint: Despite the fact that it would take more than a day to train this model from scratch, it requires far fewer epochs to train it with transfer learning than you might suspect! You probably have more fingers and toes than the number of epochs you will need!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JsaRFlZ9B6WK",
    "outputId": "cea0615f-51c1-4fbf-fac1-3f2fd51be3e0"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20  \n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hd94CKImf8vi"
   },
   "source": [
    "Did you hit you accuracy goal? Did you overshoot and spend some extra time training? When might you have been able to quit? Lets take a look at the learning curves of the training and validation accuracy/loss to analyze our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "53OTCh3jnbwV",
    "outputId": "6be9d0d5-29d1-42d7-b668-e85ab8fbdeee"
   },
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "# plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6cWgjgfrsn5"
   },
   "source": [
    "### Evaluate your model\n",
    "\n",
    "The last thing we need to do is check if the model is overfitting or if it actually learned the problem that fast! Does you model still perform well on the test set? Lets find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2KyNhagHwfar",
    "outputId": "ddc680a6-ea91-4b66-f1f3-d697e88ee6c1"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(val_dataset)\n",
    "print('Test accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UjS5ukZfOcR"
   },
   "source": [
    "And assuming your model passed the accuracy threshold you are now are all set to use this model to predict if the person exists or not. Lets print the results from a bunch of the images in our test dataset!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 898
    },
    "id": "RUNoQNgtfNgt",
    "outputId": "faca8cc6-bb18-4e6e-be98-87f03acff9c2"
   },
   "outputs": [],
   "source": [
    "#Retrieve a batch of images from the test set\n",
    "image_batch, label_batch = val_dataset.as_numpy_iterator().next()\n",
    "predictions = model.predict_on_batch(image_batch).flatten()\n",
    "\n",
    "predictions = tf.where(predictions < 0.5, 0, 1)\n",
    "\n",
    "print('Predictions:\\n', predictions.numpy())\n",
    "print('Labels:\\n', label_batch.reshape(-1).astype(\"uint8\"))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "  ax = plt.subplot(3, 3, i + 1)\n",
    "  plt.imshow(image_batch[i].astype(\"uint8\"))\n",
    "  plt.title(class_names[predictions[i]])\n",
    "  plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TZTwG7nhm0C"
   },
   "source": [
    "## Summary\n",
    "\n",
    "* **Using transfer learning for Person Detection**: In this lab, we learned how we can use transfer learning to detect if a person exists or not. When working with a small dataset, it is a common practice to take advantage of features learned by a model trained on a larger dataset in the same domain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original float model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT] # dynamic range quantization. size reduction but computing is done in fp32\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "import pathlib\n",
    "tflite_model_file = pathlib.Path('model/model-float32.tflite')\n",
    "tflite_model_file.write_bytes(tflite_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]  \n",
    "\n",
    "def representative_data_get():\n",
    "    for input_value, _ in val_dataset.take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "converter.representative_dataset = representative_data_get\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "tflite_model_quant = converter.convert()\n",
    "tflite_model_quant_file = pathlib.Path('model/model-int8.tflite')\n",
    "tflite_model_quant_file.write_bytes(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = val_dataset.as_numpy_iterator().next()\n",
    "img = image_batch[0,:,:,:]\n",
    "print (img.flatten())\n",
    "\n",
    "# Write a C source file of the image for debugging purposes (to check if the model output is correct)\n",
    "with open('src/img.h', 'w') as f:\n",
    "    f.write('#ifndef IMG_TEST_DATA\\n')\n",
    "    f.write('#define IMG_TEST_DATA\\n')\n",
    "    f.write('const float img_data[] = {')\n",
    "    for i in range(0, len(img.flatten())):\n",
    "        f.write(str(img.flatten()[i]))\n",
    "        if i < len(img.flatten()) - 1:\n",
    "            f.write(',')    \n",
    "    f.write('};\\n')\n",
    "    f.write('#endif\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_quant_file))\n",
    "interpreter.allocate_tensors()\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "interpreter.set_tensor(input_index, np.expand_dims(img, axis=0))\n",
    "interpreter.invoke()\n",
    "output = interpreter.get_tensor(output_index)\n",
    "print(output.flatten())\n",
    "if output.flatten()[0] >= 0.5:\n",
    "    print(\"Person\")\n",
    "else:\n",
    "    print(\"No person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img.astype(\"uint8\"))\n",
    "print(img.astype(\"uint8\").shape)\n",
    "print(img.astype(\"uint8\").flatten()[:3])\n",
    "print(img.astype(\"float32\").flatten()[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(interpreter):\n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    prediction_digits = []\n",
    "    test_labels = []\n",
    "\n",
    "    for test_image, test_label in tqdm(val_dataset):\n",
    "        for i in range(len(test_label)):\n",
    "            interpreter.set_tensor(input_index, np.expand_dims(test_image[i], axis=0))\n",
    "            interpreter.invoke()\n",
    "            prediction_digits.append(interpreter.get_tensor(output_index))\n",
    "            test_labels.append(test_label[i])\n",
    "    prediction_digits = np.array(prediction_digits).flatten().reshape(-1, 1)\n",
    "    print(prediction_digits[:10])\n",
    "    test_labels = np.array(test_labels)\n",
    "    print(test_labels[:10])\n",
    "    prediction_digits = np.where(prediction_digits < 0.5, 0, 1)\n",
    "    print(prediction_digits[:10])\n",
    "\n",
    "    accuracy = (prediction_digits == test_labels).mean()\n",
    "    return accuracy\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "interpreter.allocate_tensors()\n",
    "test_accuracy = evaluate_model(interpreter)\n",
    "\n",
    "print('Float model accuracy = ', test_accuracy)\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_quant_file))\n",
    "interpreter.allocate_tensors()\n",
    "test_accuracy = evaluate_model(interpreter)\n",
    "\n",
    "print('Quantized model accuracy = ', test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate model CC file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'model-int8'\n",
    "!xxd -i model/{model_file}.tflite | sed \"s/unsigned/const unsigned/g\" | sed \"s/model_model_int8_tflite/gmodel/g\"> src/model.cc\n",
    "# !xxd -n gmodel -i model/{model_file}.tflite | sed \"s/unsigned/const unsigned/g\" > src/model.cc"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
